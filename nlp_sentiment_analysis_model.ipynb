{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPL7UQS/oSZC6FpvTcouxMn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HemendraSridhar/NLP/blob/main/nlp_sentiment_analysis_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hello!, Welcome to my NLP+Finance Project.\n",
        "\n",
        "This is my Financial Sentiment Analysis Model that classifies financial text as positive, negative or neutral."
      ],
      "metadata": {
        "id": "BIlsa7Ae3_Si"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing all the required modules\n",
        "import tensorflow as tf\n",
        "!pip install transformers\n",
        "import os\n",
        "import gdown\n",
        "import torch\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import transformers\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils import pad_sequences\n",
        "from sklearn import metrics\n",
        "\n",
        "from transformers import BertTokenizer\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from transformers import BertForSequenceClassification, BertConfig\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from torch.optim import AdamW\n",
        "\n",
        "%matplotlib inline\n",
        "from google.colab import userdata\n",
        "\n",
        "# gdown.download('drivelink1', 'finance_train.csv', True)\n",
        "# gdown.download('drivelink2', 'finance_test.csv', True)\n",
        "\n",
        "testurl = userdata.get('financetest_csvfile')\n",
        "trainurl = userdata.get('financetrain_csvfile')\n",
        "\n",
        "!wget \"$testurl\" -O finance_test.csv\n",
        "!wget \"$trainurl\" -O finance_train.csv\n",
        "\n",
        "\n",
        "\n",
        "def get_finance_train():\n",
        "  df_train = pd.read_csv(\"finance_train.csv\")\n",
        "  return df_train\n",
        "def get_finance_test():\n",
        "  df_test = pd.read_csv(\"finance_test.csv\")\n",
        "  return df_test\n",
        "\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "\n",
        "print (\"Train and Test Files Loaded as train.csv and test.csv\")\n",
        "\n",
        "LABEL_MAP = {0 : \"negative\", 1 : \"neutral\", 2 : \"positive\"}\n",
        "NONE = 4 * [None]\n",
        "RND_SEED=2020\n",
        "\n",
        "def plot_confusion_matrix(y_true,y_predicted):\n",
        "  cm = metrics.confusion_matrix(y_true, y_predicted)\n",
        "  print (\"Plotting the Confusion Matrix\")\n",
        "  labels = [\"Negative\",\"Neutral\",\"Positive\"]\n",
        "  df_cm = pd.DataFrame(cm,index =labels,columns = labels)\n",
        "  fig = plt.figure(figsize=(7,6))\n",
        "  res = sns.heatmap(df_cm, annot=True,cmap='Blues', fmt='g')\n",
        "  plt.yticks([0.5,1.5,2.5], labels,va='center')\n",
        "  plt.title('Confusion Matrix - TestData')\n",
        "  plt.ylabel('True label')\n",
        "  plt.xlabel('Predicted label')\n",
        "  plt.show()\n",
        "  plt.close()"
      ],
      "metadata": {
        "id": "FprmGVq4-Gqp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = get_finance_train()\n",
        "df_test = get_finance_test()\n",
        "sentences = df_train['Sentence'].values\n",
        "labels = df_train['Label'].values"
      ],
      "metadata": {
        "id": "DmLvtv6nPNLq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenization and Formatting\n",
        "#Creating the tokens\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case = True)\n",
        "print(\"Number of unique tokens in the BERT model: \", tokenizer.vocab_size)\n",
        "\n",
        "#Text formatting adding [CLS] and [SEP], Padding and truncating and then adding an attention mask\n",
        "sentences_with_special_tokens = []\n",
        "for sentence in sentences:\n",
        "  new_sentence = \"[CLS] \" + sentence + \" [SEP]\"\n",
        "  sentences_with_special_tokens.append(new_sentence)\n",
        "\n",
        "#Tokenizing\n",
        "tokenized_texts = []\n",
        "for sentence in sentences_with_special_tokens:\n",
        "  tokenized_sentence = tokenizer.tokenize(sentence)\n",
        "  tokenized_texts.append(tokenized_sentence)\n",
        "\n",
        "#Mapping tokens to their indexes\n",
        "input_ids = []\n",
        "for text in tokenized_texts:\n",
        "  new_list = tokenizer.convert_tokens_to_ids(text)\n",
        "  input_ids.append(new_list)\n",
        "\n",
        "#Padding to a length of 128\n",
        "input_ids = pad_sequences(input_ids,maxlen=128,dtype=\"long\",truncating=\"post\",padding=\"post\")\n",
        "\n",
        "#Adding attention masks\n",
        "attention_masks = []\n",
        "for sequence in input_ids:\n",
        "  mask=[]\n",
        "  for i in sequence:\n",
        "    if i>0:\n",
        "      mask.append(1.0)\n",
        "    else:\n",
        "      mask.append(0.0)\n",
        "  attention_masks.append(mask)"
      ],
      "metadata": {
        "id": "A6LZ2WnHPfAM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Splitting Data into Train/Test Split\n",
        "X_train, X_val, y_train, y_val = train_test_split(input_ids,labels,test_size=0.15,random_state=RND_SEED)\n",
        "#Splitting attention masks\n",
        "train_masks, validation_masks, _, _ = train_test_split(attention_masks,input_ids,test_size=0.15,random_state=RND_SEED)"
      ],
      "metadata": {
        "id": "vB2UfCBGTFOu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Converting data into tensors and creating dataloaders\n",
        "train_inputs = torch.tensor(np.array(X_train));\n",
        "validation_inputs = torch.tensor(np.array(X_val));\n",
        "train_masks = torch.tensor(np.array(train_masks));\n",
        "validation_masks = torch.tensor(np.array(validation_masks));\n",
        "train_labels = torch.tensor(np.array(y_train));\n",
        "validation_labels = torch.tensor(np.array(y_val));\n",
        "\n",
        "batch_size = 32\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels);\n",
        "train_sampler = RandomSampler(train_data);\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size);\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels);\n",
        "validation_sampler = SequentialSampler(validation_data);\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size);"
      ],
      "metadata": {
        "id": "Wj54gVRETvfM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Finetuning model using BertForSequenceClassificationModel\n",
        "model = BertForSequenceClassification.from_pretrained( \"bert-base-uncased\", num_labels = 3, output_attentions = False, output_hidden_states = False);\n",
        "\n",
        "#Specifying GPU usage for pytorch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_gpu = torch.cuda.device_count()\n",
        "\n",
        "if tf.test.gpu_device_name() == '/device:GPU:0':\n",
        "  model.cuda();"
      ],
      "metadata": {
        "id": "43srcwiGT-0l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Initializing hyperparameters - learning rate and number of epochs\n",
        "optimizer = AdamW(model.parameters(),lr = 2e-5,eps = 1e-8)\n",
        "epochs = 4\n",
        "\n",
        "#TRAINING THE MODEL\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "#Total number of training steps is [number of batches] x [number of epochs]\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "#Creating the learning rate scheduler\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer,num_warmup_steps = 0,num_training_steps = total_steps)\n",
        "\n",
        "training_loss = []\n",
        "validation_loss = []\n",
        "training_stats = []\n",
        "for epoch_i in range(0, epochs):\n",
        "    #Training\n",
        "    print('Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training the model')\n",
        "    #Resetting the total loss for  epoch\n",
        "    total_train_loss = 0\n",
        "    #Putting the model into training mode\n",
        "    model.train()\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        #Progress update every 40 batches.\n",
        "        if step % 20 == 0 and not step == 0:\n",
        "            #Reporting progress\n",
        "            print('  Batch {:>5,}  of  {:>5,}. '.format(step, len(train_dataloader)))\n",
        "\n",
        "        #STEP 1 and 2: Unpacking this training batch from our dataloader\n",
        "        # While unpacking the batch, also copying each tensor to the GPU using the\n",
        "        # `to` method.`batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids\n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        #STEP 3\n",
        "        # Clearing any previously calculated gradients before performing a backward pass\n",
        "        model.zero_grad()\n",
        "\n",
        "        #STEP 4\n",
        "        # Performing a forward pass (also evaluating the model on this training batch)\n",
        "        # It returns the loss as the labels are provided\n",
        "        # the \"logits\" are the model outputs prior to activation\n",
        "        outputs = model(b_input_ids,token_type_ids=None,attention_mask=b_input_mask,labels=b_labels)\n",
        "        loss = outputs[0]\n",
        "        logits = outputs[1]\n",
        "\n",
        "        #adding the training loss over all of the batches to calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        #single value; the .item() function only returns the Python value from the tensor\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        #STEP 5\n",
        "        # Performing a backward pass to calculate the gradients\n",
        "        loss.backward()\n",
        "        # Clipping the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        #STEP 6\n",
        "        # Updating parameters and taking a step using the computed gradient\n",
        "        optimizer.step()\n",
        "        # Updating the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    #Calculating the average loss over all of the batches.\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "\n",
        "    print(\"Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "\n",
        "    #Validation\n",
        "    #After the completion of each training epoch, we measure the performance on the validation set.\n",
        "    print(\"Evaluating on Validation Set\")\n",
        "    #Putting the model in evaluation mode\n",
        "    model.eval()\n",
        "    #Tracking variables\n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "\n",
        "    #Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        #Step 1 and Step 2\n",
        "        #Unpacking this validation batch from our dataloader\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        #Telling pytorch not to construct the compute graph during the forward pass, since its only needed for backpropogation\n",
        "        with torch.no_grad():\n",
        "\n",
        "            #Forward pass, calculate logit(here they are outputs) predictions.\n",
        "            #values prior to applying an activation function like the softmax.\n",
        "            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
        "            loss = outputs[0]\n",
        "            logits = outputs[1]\n",
        "\n",
        "        #adding the validation loss.\n",
        "        total_eval_loss += loss.item()\n",
        "\n",
        "        #Moving logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        #Calculating accuracy for this batch of test sentences, and add it over all batches\n",
        "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
        "\n",
        "\n",
        "    #Final accuracy for this validation run\n",
        "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
        "    print(\"Validation Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "\n",
        "    #Calculating the average loss over all of the batches.\n",
        "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "    print(\"Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "\n",
        "    training_loss.append(avg_train_loss)\n",
        "    validation_loss.append(avg_val_loss)\n",
        "\n",
        "    #Record all statistics from this epoch.\n",
        "    training_stats.append({'epoch': epoch_i + 1, 'Training Loss': avg_train_loss, 'Valid. Loss': avg_val_loss, 'Valid. Accur.': avg_val_accuracy})\n",
        "\n",
        "print(\"Training complete!\")"
      ],
      "metadata": {
        "id": "kXqAue01Uutr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plotting the losses\n",
        "fig = plt.figure(figsize=(12,6))\n",
        "plt.title('Loss over Time')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "\n",
        "plt.plot(training_loss, label='train')\n",
        "plt.plot(validation_loss, label='validation')\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Yku-Paibkbna"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluating Test Set Accuracy\n",
        "test_sentences = df_test.Sentence.values\n",
        "test_labels = df_test.Label.values"
      ],
      "metadata": {
        "id": "JM8r-NknlMgB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Formatting Test input data\n",
        "test_input_ids, test_attention_masks = [], []\n",
        "\n",
        "#Adding Special Tokens ([CLS] and [SEP])\n",
        "formatted_sentences = []\n",
        "for sentence in test_sentences:\n",
        "  new_sent = \"[CLS] \" + sentence + \" [SEP]\"\n",
        "  formatted_sentences.append(new_sent)\n",
        "test_sentences = formatted_sentences\n",
        "\n",
        "#Tokenizing sentences\n",
        "tokenized_test_sentences = []\n",
        "for sent in test_sentences:\n",
        "  tokens = tokenizer.tokenize(sent)\n",
        "  tokenized_test_sentences.append(tokens)\n",
        "\n",
        "#Encoding Tokens to Word IDs\n",
        "test_input_ids = []\n",
        "for txt in tokenized_test_sentences:\n",
        "   ids = tokenizer.convert_tokens_to_ids(txt)\n",
        "   test_input_ids.append(ids)\n",
        "\n",
        "#Padding the inputs\n",
        "test_input_ids = pad_sequences(test_input_ids, maxlen=128, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "\n",
        "#Creating Attention Masks\n",
        "for sequence in test_input_ids:\n",
        "  mask = []\n",
        "  for i in sequence:\n",
        "    if i>0:\n",
        "      mask.append(1.0)\n",
        "    else:\n",
        "      mask.append(0.0)\n",
        "  test_attention_masks.append(mask)"
      ],
      "metadata": {
        "id": "zbF08RnFlgq-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Converting Data to Tensors and create Dataloaders\n",
        "batch_size = 32\n",
        "test_input_ids = torch.tensor(test_input_ids)\n",
        "test_attention_masks = torch.tensor(test_attention_masks)\n",
        "test_labels = torch.tensor(test_labels)\n",
        "prediction_data = TensorDataset(test_input_ids, test_attention_masks, test_labels)\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "PvrRlS1Frc3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluating Test Set Accuracy\n",
        "\n",
        "#Prediction on test set\n",
        "print('Predicting labels for {:,} test sentences...'.format(len(test_input_ids)))\n",
        "\n",
        "#Putting model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "#Tracking variables\n",
        "predictions , true_labels = [], []\n",
        "\n",
        "#Prediction\n",
        "for batch in prediction_dataloader:\n",
        "  #Adding batch to GPU\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "\n",
        "  #Unpack the inputs from our dataloader\n",
        "  b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "  with torch.no_grad():\n",
        "      #Forward pass, calculating logit predictions\n",
        "      outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
        "\n",
        "  logits = outputs[0]\n",
        "\n",
        "  #Moving logits and labels to CPU\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "  #Storing predictions and true labels\n",
        "  predictions.append(logits)\n",
        "  true_labels.append(label_ids)\n",
        "\n",
        "#Final tracking variables\n",
        "y_logits, y_true, y_preds = [], [], []\n",
        "\n",
        "#Appending logit predictions\n",
        "for chunk in predictions:\n",
        "  for logits in chunk:\n",
        "    y_logits.append(logits)\n",
        "\n",
        "#Appending true labels\n",
        "for chunk in true_labels:\n",
        "  for label in chunk:\n",
        "    y_true.append(label)\n",
        "\n",
        "#Appending real predictions\n",
        "for logits in y_logits:\n",
        "  y_preds.append(np.argmax(logits))\n",
        "\n",
        "print('Test Accuracy: {:.2%}'.format(metrics.accuracy_score(y_preds,y_true)))\n",
        "plot_confusion_matrix(y_true,y_preds)"
      ],
      "metadata": {
        "id": "teV3Y1ScruuP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#THANK YOU FOR READING!"
      ],
      "metadata": {
        "id": "mMx_Xtj6t2eO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
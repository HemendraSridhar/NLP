{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNraNxJ4KNpfYusmEiqymAK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HemendraSridhar/NLP/blob/main/NLP_Sentiment_Analysis_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hello!, Welcome to my NLP+Finance Project.\n",
        "\n",
        "This is my Financial Sentiment Analysis Model that classifies financial text as positive, negative or neutral."
      ],
      "metadata": {
        "id": "BIlsa7Ae3_Si"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing all the required modules\n",
        "import tensorflow as tf\n",
        "!pip install transformers\n",
        "import os\n",
        "import gdown\n",
        "import torch\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import transformers\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils import pad_sequences\n",
        "from sklearn import metrics\n",
        "\n",
        "from transformers import BertTokenizer\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from transformers import BertForSequenceClassification, BertConfig\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from torch.optim import AdamW\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "# gdown.download('https://drive.google.com/uc?id=1q4U2gVY9tWEPdT6W-pdQpKmo152QqWLE', 'finance_train.csv', True)\n",
        "# gdown.download('https://drive.google.com/uc?id=1nIBqAsItwVEGVayYTgvybz7HeK0asom0', 'finance_test.csv', True)\n",
        "\n",
        "!wget 'https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%206%20-%2010%20(Projects)/Project%20-%20NLP%2BFinance/finance_test.csv'\n",
        "!wget 'https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%206%20-%2010%20(Projects)/Project%20-%20NLP%2BFinance/finance_train.csv'\n",
        "\n",
        "def get_finance_train():\n",
        "  df_train = pd.read_csv(\"finance_train.csv\")\n",
        "  return df_train\n",
        "def get_finance_test():\n",
        "  df_test = pd.read_csv(\"finance_test.csv\")\n",
        "  return df_test\n",
        "\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "\n",
        "print (\"Train and Test Files Loaded as train.csv and test.csv\")\n",
        "\n",
        "LABEL_MAP = {0 : \"negative\", 1 : \"neutral\", 2 : \"positive\"}\n",
        "NONE = 4 * [None]\n",
        "RND_SEED=2020\n",
        "\n",
        "def plot_confusion_matrix(y_true,y_predicted):\n",
        "  cm = metrics.confusion_matrix(y_true, y_predicted)\n",
        "  print (\"Plotting the Confusion Matrix\")\n",
        "  labels = [\"Negative\",\"Neutral\",\"Positive\"]\n",
        "  df_cm = pd.DataFrame(cm,index =labels,columns = labels)\n",
        "  fig = plt.figure(figsize=(7,6))\n",
        "  res = sns.heatmap(df_cm, annot=True,cmap='Blues', fmt='g')\n",
        "  plt.yticks([0.5,1.5,2.5], labels,va='center')\n",
        "  plt.title('Confusion Matrix - TestData')\n",
        "  plt.ylabel('True label')\n",
        "  plt.xlabel('Predicted label')\n",
        "  plt.show()\n",
        "  plt.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FprmGVq4-Gqp",
        "outputId": "6e218cae-5da9-40d4-fcee-87e0fa35627f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Importing all the required modules\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = get_finance_train()\n",
        "df_test = get_finance_test()"
      ],
      "metadata": {
        "id": "DmLvtv6nPNLq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenization and Formatting\n",
        "#Creating the tokens\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case = True)\n",
        "print(\"Number of unique tokens in the BERT model: \", tokenizer.vocab_size)\n",
        "\n",
        "#Text formatting adding [CLS] and [SEP], Padding and truncating and then adding an attention mask\n",
        "sentences_with_special_tokens = []\n",
        "for sentence in sentences:\n",
        "  new_sentence = \"[CLS] \" + sentence + \" [SEP]\"\n",
        "  sentences_with_special_tokens.append(new_sentence)\n",
        "\n",
        "#Tokenizing\n",
        "tokenized_texts = []\n",
        "for sentence in sentences_with_special_tokens:\n",
        "  tokenized_sentence = tokenizer.tokenize(sentence)\n",
        "  tokenized_texts.append(tokenized_sentence)\n",
        "\n",
        "#Mapping tokens to their indexes\n",
        "input_ids = []\n",
        "for text in tokenized_texts:\n",
        "  new_list = tokenizer.convert_tokens_to_ids(text)\n",
        "  input_ids.append(new_list)\n",
        "\n",
        "#Padding to a length of 128\n",
        "input_ids = pad_sequences(input_ids,maxlen=128,dtype=\"long\",truncating=\"post\",padding=\"post\")\n",
        "\n",
        "#Adding attention masks\n",
        "attention_masks = []\n",
        "for sequence in input_ids:\n",
        "  mask=[]\n",
        "  for i in sequence:\n",
        "    if i>0:\n",
        "      mask.append(1.0)\n",
        "    else:\n",
        "      mask.append(0.0)\n",
        "  attention_masks.append(mask)"
      ],
      "metadata": {
        "id": "A6LZ2WnHPfAM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Splitting Data into Train/Test Split\n",
        "X_train, X_val, y_train, y_val = train_test_split(input_ids,labels,est_size=0.15,random_state=RND_SEED)\n",
        "#Splitting attention masks\n",
        "train_masks, validation_masks, _, _ = train_test_split(attention_masks,input_ids,test_size=0.15,random_state=RND_SEED)"
      ],
      "metadata": {
        "id": "vB2UfCBGTFOu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Converting data into tensors and creating dataloaders\n",
        "train_inputs = torch.tensor(np.array(X_train));\n",
        "validation_inputs = torch.tensor(np.array(X_val));\n",
        "train_masks = torch.tensor(np.array(train_masks));\n",
        "validation_masks = torch.tensor(np.array(validation_masks));\n",
        "train_labels = torch.tensor(np.array(y_train));\n",
        "validation_labels = torch.tensor(np.array(y_val));\n",
        "\n",
        "batch_size = 32\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels);\n",
        "train_sampler = RandomSampler(train_data);\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size);\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels);\n",
        "validation_sampler = SequentialSampler(validation_data);\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size);"
      ],
      "metadata": {
        "id": "Wj54gVRETvfM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Finetuning model using BertForSequenceClassificationModel\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\",\n",
        "    num_labels = 3,\n",
        "    output_attentions = False,\n",
        "    output_hidden_states = False,\n",
        ");\n",
        "\n",
        "#Specifying GPU usage for pytorch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_gpu = torch.cuda.device_count()\n",
        "\n",
        "if tf.test.gpu_device_name() == '/device:GPU:0':\n",
        "  model.cuda();"
      ],
      "metadata": {
        "id": "43srcwiGT-0l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Initializing hyperparameters - learning rate and number of epochs\n",
        "optimizer = AdamW(model.parameters(),lr = 2e-5,eps = 1e-8)\n",
        "epochs = 4\n",
        "\n",
        "#TRAINING THE MODEL\n"
      ],
      "metadata": {
        "id": "kXqAue01Uutr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}